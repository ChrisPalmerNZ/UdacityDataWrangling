{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Project - report\n",
    "- A summary of the work performed to clean and tidy data supplied as part of the Udacity Data Analysis nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Context\n",
    "Real-world data rarely comes clean. Using Python and its libraries, you will gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. You will document your wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and visualizations using Python (and its libraries) and/or SQL.\n",
    "\n",
    "The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs). WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"[they're good dogs Brent](http://knowyourmeme.com/memes/theyre-good-dogs-brent)\". WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs [downloaded their Twitter archive](https://support.twitter.com/articles/20170160) and sent it to Udacity via email exclusively for you to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "\n",
    "**The goal:** wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning is required for \"Wow!\"-worthy analyses and visualizations.\n",
    "\n",
    "## The Data\n",
    "### Enhanced Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "\n",
    "![data_image](images/data_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Data via the Twitter API\n",
    "\n",
    "Back to the basic-ness of Twitter archives: **retweet count** and **favorite count** are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. But you, because you have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+. And guess what? You're going to query Twitter's API to gather this valuable data.\n",
    "\n",
    "### Image Predictions File\n",
    "\n",
    "One more cool thing: I ran every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "![pictures_data_image](images/pictures_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The deliverables\n",
    "Tasks in the project are as follows:\n",
    "\n",
    "- Data wrangling, which consists of:\n",
    "    - Gathering data (downloadable file in the Resources tab in the left most panel of your classroom and linked in step 1 below).\n",
    "    - Assessing data\n",
    "    - Cleaning data\n",
    "- Storing, analyzing, and visualizing your wrangled data\n",
    "- Reporting on 1) your data wrangling efforts and 2) your data analyses and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected outputs\n",
    "- `wrangle_act.ipynb`: code for gathering, assessing, cleaning, analyzing, and visualizing data\n",
    "- `wrangle_report.pdf` or `wrangle_report.html`: documentation for data wrangling steps: gather, assess, and clean\n",
    "- `act_report.pdf` or `act_report.html`: documentation of analysis and insights into final data\n",
    "- `twitter_archive_enhanced.csv`: file as given\n",
    "- `image_predictions.tsv`: file downloaded programmatically\n",
    "- `tweet_json.txt`: file constructed via API\n",
    "- `twitter_archive_master.csv`: combined and cleaned data\n",
    "- any additional files (e.g. files for additional pieces of gathered data or a database file for your stored clean data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Summary of outputs\n",
    "- All wrangling, tidying and visualization was performed, according to requirements, in accompanying notebook `wrangle_act.ipynb`\n",
    "- This is also available as `wrangle_act.html`\n",
    "- This report is the required `wrangle_report.html`\n",
    "- A separate `act_report.html` is also provided\n",
    "- The original `twitter_archive_enhanced.csv` is retained\n",
    "- Cleaned data is available as `twitter_archive_master.csv`\n",
    "- Additionally, tidied data is supplied as `dog_types.csv`, `dog_names.csv`, and `dog_breed_predictions.csv` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Cleaning performed\n",
    "- Need only original ratings that have images  \n",
    "    1. Removed retweets\n",
    "    2. Removed records without URLs\n",
    "        - This was because only URLs can lead to an image\n",
    "        - And in preference to only keeping records with a linked image prediction\n",
    "    3. Removed some records that didn't actually have dog images \n",
    "        - When their score was zero \n",
    "        - However, kept some that didn't lead to dogs (e.g. a chicken, a person, a fan) as they were compatible with the site's humor\n",
    "        - This was a proof of concept rather than an exhaustive fix\n",
    "- Fixed incorrect data\n",
    "    4. Removed duplicates from the `expanded_urls` column\n",
    "    5. Replaced the word \"None\" in `Name` and `dog types` columns (`doggo`, `floofer`, `pupper`, `puppo`) with nulls (Python `None`)\n",
    "    6. Removed invalid `Name` values (words such as 'a', 'actually', 'all', 'an', 'by', 'getting' etc.)  \n",
    "    7. Added some missing `Name` values\n",
    "        - This was a token effort\n",
    "        - A robust approach, but beyond the scope of this exercise, would be to use named entity recognition with tools like [NLTK and spaCy](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)\n",
    "    8. Corrected dog scores\n",
    "    9. Corrected invalid dog types where these were not mentioned or incorrectly calculated\n",
    "    10. Supplied missing dog types\n",
    "- Added missing columns\n",
    "    11. Fetched missing data - `retweet_count` and `favorite_count` - using the Tweepy API\n",
    "        - see python program `get_tweets_by_id.py`\n",
    "    12. Added `retweet_count` and `favorite_count` to the clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Tidying performed\n",
    "1. Extracted dog types columns `doggo`, `floofer`, `pupper` and `puppo` to a separate `df_dog_type` dataframe and exported to `dog_types.csv`\n",
    "    - Consists of `ID`,`tweet_id` and `dog_type` columns\n",
    "    - However, kept the original dog type columns, but reset the data type of the columns to boolean    \n",
    "<br>    \n",
    "2. Extracted `name` column to a separate `df_name` dataframe and exported to `dog_names.csv`\n",
    "    - Consists of `ID`,`tweet_id` and `name` columns\n",
    "    - However, kept the original `name` column, but after correcting it may now contain more than one name    \n",
    "<br>   \n",
    "3. Created a long-form version of the images table and exported to `dog_breed_predictions.csv`\n",
    "    - Consists of `tweet_id`,`score`,`isdog` and `breed` columns\n",
    "    - Did not alter the original images table, this can link to it as a parent table on tweet_id to get image file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
